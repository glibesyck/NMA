# Intro to Neurons and Synapses

One of the common questions in neuroscience is to identify the causes of changes in the statistics of spiking activity patterns. Whether these changes are caused by changes in neuron/synapse properties or by a change in the input or by a combination of both. Neurons - marvellous amount of different sizes and shapes.

Wilfrid Rall. Actual beautiful branced structure of a neuron cannot be simplified with spherical point model (space matter a lot). Cable theory - dendrites and axon impact (stimulus propagation along cable - stimulus becomes more smooth and the peak becomes delayed). Both in time and space we have exponential decay. Neuron can do interesting computations because of sequence selectivity (if we time inputs when peaks coincide, we get nice large signal as output, it is something that isn't taken into account with point neuron model). It turned out that sequence selectivity is also velocity selective, so many space for different computations to happen!

Hodgkin and Huxley. Ion channels + action potential. At first, we deliver charge to the cell, we depolarize it, Na (sodium) channel opens, then K (potassium) opens and pulls potential back and even below resting potential. Shapes of action potentials are very important! There is a bunch of different channels, one of them - Calcium dependent Potassium one (Ca). Effects of some chosen channels - to delay the time of the next action potential. bursts are important in the plasticity. Not only what are the channels but also where are they located. Squid has giant axon, it allows them to communicate (what??) a little bit faster. Myelination - strong amplification effect (layer around neuron for faster propagation with less metabolic cost).

Alan Turing. Paper on chemical computations. Information as chemistry (chemical signals convey information). Classical example of chemical signaling: logic at the level of molecules, associativitty due to NMDA receptor (acts like an AND gate, we need to have post- and pre- synaptic activities), important role of Magnesium (Mg) here.

Lifetime of a protein is a couple of days; lifetime of a neuron is 100 years. ANN are a dominant metaphor for computing but they are very inefficient comparing to the brain. Neuorns can do many key operations: summation, integration, sequence operation, association, memory. Computation occurs across scales (we don't need to dive into the most detailed level; we need to choose the appropriate one for our question).

# LIF Neuron Model

LIF = Leaky (with time it looses accumulated potential) Integrate(accumulates potential)-and-Fire(fires when reaches threshold). This model - focus on cell body and axon. Main thing of study - how different inputs affect the LIF neuronâ€™s output (firing rate and spike time irregularity).

Dendrites recieve input, cell body sums currents from dendrites and axon send to action potential. C - capacitance of the membrane; $g_L$ - conductance of the membrane; $E_L$ - equilibrium potential of the leak. Coming current - comes to capacitor and leaks. $I_{Na}, I_K, I_H, I_{AHP}$ - last two don't participate in action potential formation (we exclude them from this model); first two are replaced by the external threshold (the corresponding mechanism). At some point current flowing from the leak becomes equal to input through dendrites - plateau state. Exponential relaxation to a steady-state. For constant current, ISI is constant:) Indeed, no matter what ISI rate, the action potential shape looks pretty the same! (empirical observation). Some notable exceptions: it may burst with high-frequency input (epilepsy?).

Which features to consider while talking about Input-Output transfer function? In simple case, let's consider all currents flow through dendrites as input and output - voltage. In case of constant input, we can talk about firing frequency (everything is the same) - it's the inverse of ISI. Plot of current input and firing rate will be asymptotically logarithmic. By adding noise (to constant input), we can't define firing rate that easy but to simplify calculations - average value; noisy linearizes this curve (interesting!). The analytical equation - Siegert equation. CV - coefficient of variation (standard deviation of ISI devided by mean value of ISI), measures spike regularity. A Poisson train is an example of high irregularity, in which $\text{CV}_{\text{ISI}} = 1$. And for a clocklike (regular) process we have $\text{CV}_{\text{ISI}} = 0$ because of **std(ISI)=0**.

Fluctuation-driven regime (neuron is driven irregularly because of fluctuations) vs mean-driven regime. The variability of the responses, thus, depends on the variance as well as mean value! With bigger membrane time $\tau_m$ firing rate is decreased because the membrane needs more time to reach the threshold after the reset. If we have bigger current fluctuations (increased sigma), the minimum input needed to make a neuron spike is smaller as the fluctuations can help push the voltage above threshold. The higher the sigma the more irregular the observed spikes. If we use a DC input, the F-I curve is deterministic, and we can found its shape by solving the membrane equation of the neuron. If we have GWN, as we increase the sigma, the F-I curve has a more linear shape, and the neuron reaches its threshold using less average injected current. Noise is acting to suppress the non-linearities and render a neuron as a linear system.

# Input Correlation

Correlation is important as correlated neurons can provide bigger impact on donstream neurons. Correlation between number of spikes (output correlation) and input - shared source of noise (and each has its own). Clearly output correlation is scaling with input one but(!) we can use output one as readout for input (slope will differ, it depends on the gain of the neurons, CV of the neurons and average rate).
